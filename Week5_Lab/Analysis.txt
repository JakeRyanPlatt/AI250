In this lab, I trained three classification models on the UCI Spambase dataset to detect spam emails: Decision Tree, Random Forest, and Gradient Boosting. Overall, all models performed well, but there were clear differences. The Decision Tree achieved 91.10% accuracy, 0.8828 precision, and 0.8926 recall. While respectable, it seemed more prone to overfitting and produced the weakest precision, meaning it generated more false spam flags than the other models.

The ensemble methods performed better. The Random Forest model achieved the best overall performance with 94.46% accuracy, 0.9483 precision, and 0.9091 recall. This balance suggests it correctly identifies most spam while keeping false positives relatively low. Gradient Boosting was close behind at 93.92% accuracy, 0.9373 precision, and 0.9063 recall, indicating that both ensemble approaches are well-suited to this task.

From this project, I learned how model choice affects the tradeoff between precision and recall in a real-world problem where misclassifying emails has consequences. A challenge I faced was correctly loading and formatting the raw Spambase file from UCI and making sure the label column was handled properly. Once the data was working, comparing multiple models with consistent train/test splits made their differences much easier to interpret.
